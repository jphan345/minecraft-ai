{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jphan345/minecraft-ai/blob/main/minerl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ysSTXmT3YUeF"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hekygFxvEJkh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 --version"
      ],
      "metadata": {
        "id": "qObnXXunTni4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo apt-get update -y\n",
        "# !sudo apt-get install python3.6\n",
        "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 1\n",
        "# !sudo update-alternatives --config python3\n",
        "# !sudo apt install python3-pip\n",
        "\n",
        "# import tarfile\n",
        "# my_tar = tarfile.open('./drive/MyDrive/Colab Notebooks/minerl-0.4.4.tar.gz')\n",
        "# my_tar.extractall('./') # specify which folder to extract to\n",
        "# my_tar.close()\n",
        "\n",
        "# wait(100)\n",
        "# !python3 ./minerl-0.4.4/setup.py install"
      ],
      "metadata": {
        "id": "K-nxjqiUxnvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HTScYNljgXv"
      },
      "source": [
        "%%capture\n",
        "!sudo add-apt-repository -y ppa:openjdk-r/ppa\n",
        "!sudo apt-get purge openjdk-*\n",
        "!sudo apt-get install openjdk-8-jdk\n",
        "!sudo apt-get install xvfb xserver-xephyr vnc4server python-opengl ffmpeg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "xvn7I-Ey1jX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh6gb3UWjT3p"
      },
      "source": [
        "%%capture\n",
        "!pip3 install --upgrade minerl==0.3.7\n",
        "!pip3 install pyvirtualdisplay\n",
        "!pip3 install pytorch\n",
        "!pip3 install scikit-learn\n",
        "!pip3 install -U colabgymrender"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo apt install ffmpeg\n",
        "!pip3 install imageio==2.4.1"
      ],
      "metadata": {
        "id": "gjivJZBxz5TI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip3 install --upgrade numpy==1.21.6\n",
        "# !pip3 uninstall minerl\n",
        "# !pip3 install minerl==0.3.7"
      ],
      "metadata": {
        "id": "3B9eVEjKCDk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADmrUKxvYXGa"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8_vZpMFpiD9"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import gym\n",
        "import minerl\n",
        "from tqdm.notebook import tqdm\n",
        "from colabgymrender.recorder import Recorder\n",
        "from pyvirtualdisplay import Display\n",
        "from sklearn.cluster import KMeans\n",
        "import logging\n",
        "import math\n",
        "logging.disable(logging.ERROR) # reduce clutter, remove if something doesn't work to see the error logs."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKiasaipYa6l"
      },
      "source": [
        "# Traditional CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyOxGuA5At1g"
      },
      "source": [
        "class NatureCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    CNN from DQN nature paper:\n",
        "        Mnih, Volodymyr, et al.\n",
        "        \"Human-level control through deep reinforcement learning.\"\n",
        "        Nature 518.7540 (2015): 529-533.\n",
        "\n",
        "    :param input_shape: A three-item tuple telling image dimensions in (C, H, W)\n",
        "    :param output_dim: Dimensionality of the output vector\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "        n_input_channels = input_shape[0]\n",
        "        # CNN layers\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "\n",
        "        # Compute shape by doing one forward pass\n",
        "        with th.no_grad():\n",
        "            n_flatten = self.cnn(th.zeros(1, *input_shape)).shape[1]\n",
        "\n",
        "        self.linear = nn.Sequential(\n",
        "            nn.Linear(n_flatten, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        return self.linear(self.cnn(observations))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ImpalaCNN\n",
        "\n",
        "Larger network than NatureCNN: adding residual blocks and similar techniques can improve performance of deep RL agents. https://arxiv.org/abs/1802.01561"
      ],
      "metadata": {
        "id": "xjVHql627H73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImpalaCNN(nn.Module):\n",
        "    def __init__(self, input_shape, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        depth_in = input_shape[0]\n",
        "        filter_sizes = (32, 64, 64)\n",
        "        # Scaler for FixUp mid-most convolutions.\n",
        "        first_conv_weight_scale = 1 / (math.sqrt(len(filter_sizes) * 2))\n",
        "\n",
        "        for depth_out in filter_sizes:\n",
        "            layers.extend([\n",
        "                nn.Conv2d(depth_in, depth_out, kernel_size=3, stride=1, padding=1),\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n",
        "                FixupResidual(depth_out, first_conv_weight_scale),\n",
        "                FixupResidual(depth_out, first_conv_weight_scale),\n",
        "            ])\n",
        "            depth_in = depth_out\n",
        "        # Extra residual layer without max pooling\n",
        "        layers.extend([\n",
        "            FixupResidual(depth_in, first_conv_weight_scale),\n",
        "            FixupResidual(depth_in, first_conv_weight_scale),\n",
        "        ])\n",
        "\n",
        "        self.conv_layers = nn.Sequential(*layers)\n",
        "        self.linear = nn.Linear(filter_sizes[-1] *\n",
        "                                math.ceil(input_shape[1] ** (1/2)) *\n",
        "                                math.ceil(input_shape[2] ** (1/2)),\n",
        "                                output_dim)\n",
        "\n",
        "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
        "        # Transpose observations to be channel-first (BCHW instead of BHWC)\n",
        "        # observations shape: (batch_size, height, width, channels)\n",
        "        # transpose to: (batch_size, channels, height, width)\n",
        "        observations = observations.permute(0, 3, 1, 2).contiguous()\n",
        "        # Normalize observations. Do this here to avoid using too much memory (images are uint8 by default)\n",
        "        observations /= 255.0\n",
        "        x = self.conv_layers(observations)\n",
        "        x = F.relu(x)\n",
        "        x = x.view(x.shape[0], -1)\n",
        "        x = self.linear(x)\n",
        "        x = F.relu(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ImpalaResidual(nn.Module):\n",
        "    \"\"\"\n",
        "    A residual block for an IMPALA CNN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, depth):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(x)\n",
        "        out = self.conv1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        return out + x\n",
        "\n",
        "class FixupResidual(nn.Module):\n",
        "    def __init__(self, depth, first_conv_weight_scale):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.conv2 = nn.Conv2d(depth, depth, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        self.bias1 = nn.Parameter(th.zeros([depth, 1, 1]))\n",
        "        self.bias2 = nn.Parameter(th.zeros([depth, 1, 1]))\n",
        "        self.bias3 = nn.Parameter(th.zeros([depth, 1, 1]))\n",
        "        self.bias4 = nn.Parameter(th.zeros([depth, 1, 1]))\n",
        "        self.scale = nn.Parameter(th.ones([depth, 1, 1]))\n",
        "\n",
        "        # Final Convs in residual branches initializedto zero\n",
        "        # Other convs in residual branches initialized to a scaled value\n",
        "        with th.no_grad():\n",
        "            self.conv2.weight *= 0\n",
        "            self.conv1.weight *= first_conv_weight_scale\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(x)\n",
        "\n",
        "        out = x + self.bias1\n",
        "        out = self.conv1(out)\n",
        "        out = out + self.bias2\n",
        "\n",
        "        out = F.relu(out)\n",
        "\n",
        "        out = out + self.bias3\n",
        "        out = self.conv2(out)\n",
        "        out = out * self.scale\n",
        "        out = out + self.bias4\n",
        "\n",
        "        return out + x"
      ],
      "metadata": {
        "id": "0xR5_UFaLStP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvrJks0gZCTW"
      },
      "source": [
        "# Setup training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpH8vzpLBGRY"
      },
      "source": [
        "def train():\n",
        "    # We will only use ObtainPickaxe data which is smaller,\n",
        "    # but has the similar steps as ObtainDiamond in the beginning.\n",
        "    # \"VectorObf\" stands for vectorized (vector observation and action), where there is no\n",
        "    # clear mapping between original actions and the vectors (i.e. AI needs to learn it)\n",
        "    iron_pick_data = minerl.data.make(\"MineRLObtainIronPickaxeVectorObf-v0\",  data_dir='data', num_workers=1)\n",
        "    treechop_data = minerl.data.make(\"MineRLTreechopVectorObf-v0\",  data_dir='data', num_workers=1)\n",
        "    # diamond_data = minerl.data.make(\"MineRLObtainDiamondVectorObf-v0\",  data_dir='data', num_workers=1)\n",
        "\n",
        "    datasets = [iron_pick_data, treechop_data];\n",
        "\n",
        "    # Use k-means to find actions that represent most of them.\n",
        "    # Go over the dataset once and collect all actions and the observations (the \"pov\" image).\n",
        "    # We do this to later on have uniform sampling of the dataset and to avoid high memory use spikes.\n",
        "    all_actions = []\n",
        "    all_pov_obs = []\n",
        "\n",
        "    print(\"Loading data\")\n",
        "    for i in range(len(datasets)):\n",
        "      data = datasets[i]\n",
        "      trajectory_names = data.get_trajectory_names()\n",
        "      random.shuffle(trajectory_names)\n",
        "\n",
        "      # Add trajectories to the data until we reach the required DATA_SAMPLES.\n",
        "      for trajectory_name in trajectory_names:\n",
        "          trajectory = data.load_data(trajectory_name, skip_interval=0, include_metadata=False)\n",
        "          for dataset_observation, dataset_action, _, _, _ in trajectory:\n",
        "              all_actions.append(dataset_action[\"vector\"])\n",
        "              all_pov_obs.append(dataset_observation[\"pov\"])\n",
        "          if len(all_actions) >= (DATA_SAMPLES / len(datasets)) * (i + 1):\n",
        "              break\n",
        "\n",
        "    all_actions = np.array(all_actions)\n",
        "    all_pov_obs = np.array(all_pov_obs)\n",
        "\n",
        "    # Run k-means clustering using scikit-learn.\n",
        "    print(\"Running KMeans on the action vectors\")\n",
        "    kmeans = KMeans(n_clusters=NUM_ACTION_CENTROIDS)\n",
        "    kmeans.fit(all_actions)\n",
        "    action_centroids = kmeans.cluster_centers_\n",
        "    print(\"KMeans done\")\n",
        "\n",
        "    # Do behavioural cloning on the discrete actions, where we turn the\n",
        "    # original vectors into discrete choices by mapping them to the closest\n",
        "    # centroid (based on Euclidian distance).\n",
        "\n",
        "    network = ImpalaCNN((3, 64, 64), NUM_ACTION_CENTROIDS).cuda()\n",
        "    optimizer = th.optim.Adam(network.parameters(), lr=LEARNING_RATE)\n",
        "    loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "    num_samples = all_actions.shape[0]\n",
        "    update_count = 0\n",
        "    losses = []\n",
        "    # We have the data loaded up already in all_actions and all_pov_obs arrays.\n",
        "    # Training loop\n",
        "    print(\"Training\")\n",
        "    for _ in range(EPOCHS):\n",
        "        # Randomize the order in which we go over the samples\n",
        "        epoch_indices = np.arange(num_samples)\n",
        "        np.random.shuffle(epoch_indices)\n",
        "        for batch_i in range(0, num_samples, BATCH_SIZE):\n",
        "            # NOTE: this will cut off incomplete batches from end of the random indices\n",
        "            batch_indices = epoch_indices[batch_i:batch_i + BATCH_SIZE]\n",
        "\n",
        "            # Load the inputs and preprocess\n",
        "            obs = all_pov_obs[batch_indices].astype(np.float32)\n",
        "            # # Transpose observations to be channel-first (BCHW instead of BHWC)\n",
        "            # obs = obs.transpose(0, 3, 1, 2)\n",
        "            # # Normalize observations. Do this here to avoid using too much memory (images are uint8 by default)\n",
        "            # obs /= 255.0\n",
        "\n",
        "            # Map actions to their closest centroids\n",
        "            action_vectors = all_actions[batch_indices]\n",
        "            # Use numpy broadcasting to compute the distance between all\n",
        "            # actions and centroids at once.\n",
        "            # \"None\" in indexing adds a new dimension that allows the broadcasting\n",
        "            distances = np.sum((action_vectors - action_centroids[:, None]) ** 2, axis=2)\n",
        "            # Get the index of the closest centroid to each action.\n",
        "            # This is an array of (batch_size,)\n",
        "            actions = np.argmin(distances, axis=0)\n",
        "\n",
        "            # Obtain logits of each action\n",
        "            logits = network(th.from_numpy(obs).float().cuda())\n",
        "\n",
        "            # Minimize cross-entropy with target labels.\n",
        "            # We could also compute the probability of demonstration actions and\n",
        "            # maximize them.\n",
        "            loss = loss_function(logits, th.from_numpy(actions).long().cuda())\n",
        "\n",
        "            # Standard PyTorch update\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            update_count += 1\n",
        "            losses.append(loss.item())\n",
        "            if (update_count % 1000) == 0:\n",
        "                mean_loss = sum(losses) / len(losses)\n",
        "                tqdm.write(\"Iteration {}. Loss {:<10.3f}\".format(update_count, mean_loss))\n",
        "                losses.clear()\n",
        "\n",
        "            # Start saving the model after 1 mil iterations just incase of a crash\n",
        "            if (update_count > 1000000 and update_count % 100000):\n",
        "                np.save(TRAIN_KMEANS_MODEL_NAME, action_centroids)\n",
        "                th.save(network.state_dict(), TRAIN_MODEL_NAME)\n",
        "    print(\"Training done\")\n",
        "\n",
        "    # Save network and the centroids into separate files\n",
        "    np.save(TRAIN_KMEANS_MODEL_NAME, action_centroids)\n",
        "    th.save(network.state_dict(), TRAIN_MODEL_NAME)\n",
        "    for data in datasets:\n",
        "        del data\n",
        "    del datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fg68dO21ZsgG"
      },
      "source": [
        "\n",
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5VCVeHyqDlm"
      },
      "source": [
        "# Parameters:\n",
        "# epochs = 256\n",
        "# batch_size = 32\n",
        "# lr = 0.0000625\n",
        "EPOCHS = 32  # how many times we train over dataset.\n",
        "LEARNING_RATE = 0.0000625  # Learning rate for the neural network.\n",
        "BATCH_SIZE = 32\n",
        "NUM_ACTION_CENTROIDS = 60  # Number of KMeans centroids used to cluster the data.\n",
        "\n",
        "DATA_SAMPLES = 1000000  # how many samples to use from the dataset. Impacts RAM usage\n",
        "\n",
        "TRAIN_MODEL_NAME = './drive/MyDrive/Colab Notebooks/trained_agent.pth'  # name to use when saving the trained agent.\n",
        "TEST_MODEL_NAME = './drive/MyDrive/Colab Notebooks/trained_agent.pth'  # name to use when loading the trained agent.\n",
        "TRAIN_KMEANS_MODEL_NAME = './drive/MyDrive/Colab Notebooks/kmeans_model.npy'  # name to use when saving the KMeans model.\n",
        "TEST_KMEANS_MODEL_NAME = './drive/MyDrive/Colab Notebooks/kmeans_model.npy'  # name to use when loading the KMeans model.\n",
        "\n",
        "TEST_EPISODES = 25  # number of episodes to test the agent for.\n",
        "MAX_TEST_EPISODE_LEN = 6000  # 18k is the default for MineRLObtainDiamondVectorObf."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lumAopy0cHBM"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzD13IclpD4T"
      },
      "source": [
        "import os\n",
        "\n",
        "data_paths = ['./data/MineRLObtainIronPickaxeVectorObf-v0',\n",
        "              './data/MineRLTreechopVectorObf-v0',\n",
        "              './data/MineRLObtainDiamondVectorObf-v0']\n",
        "\n",
        "# Don't downloadMineRLObtainDiamondVectorObf-v0 to save time since we are not using it\n",
        "for path in data_paths[:-1]:\n",
        "  if not os.path.exists(path):\n",
        "    minerl.data.download(directory='data', environment=path[7:])\n",
        "  else:\n",
        "    print(f\"{path[7:]} already exists!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zKLHW_JcRBJ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!apt-get update\n",
        "!apt-get install -y xvfb"
      ],
      "metadata": {
        "id": "0Q902gXU26Ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9OKQCgz4XQk"
      },
      "source": [
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH84zVpiB19e"
      },
      "source": [
        "train()  # only need to run this once."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_6j2-ibcxZh"
      },
      "source": [
        "# Start Minecraft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja8WN5rgc3Ex"
      },
      "source": [
        "env = gym.make('MineRLObtainDiamondDenseVectorObf-v0')\n",
        "env = Recorder(env, './video', fps=60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjKpJgZicn6L"
      },
      "source": [
        "# Run your agent\n",
        "As the code below runs you should see episode videos and rewards show up. You can run the below cell multiple times to see different episodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQeH2QgcjpK9"
      },
      "source": [
        "action_centroids = np.load(TEST_KMEANS_MODEL_NAME)\n",
        "network = ImpalaCNN((3, 64, 64), NUM_ACTION_CENTROIDS).cuda()\n",
        "network.load_state_dict(th.load(TEST_MODEL_NAME))\n",
        "\n",
        "\n",
        "num_actions = action_centroids.shape[0]\n",
        "action_list = np.arange(num_actions)\n",
        "total_rewards = []\n",
        "\n",
        "for episode in range(TEST_EPISODES):\n",
        "    rewards = []\n",
        "    env.seed(95)  # https://drive.google.com/file/d/1JawPwdfOyxTaYeF7imhhXp2VJR2wMnzk/view\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "\n",
        "    while not done:\n",
        "        # Process the action:\n",
        "        #   - Add/remove batch dimensions\n",
        "        #   - Transposing and normalizing done in network\n",
        "        # obs = th.from_numpy(obs['pov'].transpose(2, 0, 1)[None].astype(np.float32) / 255).cuda()\n",
        "        obs = th.from_numpy(obs['pov'][None].astype(np.float32)).cuda()\n",
        "        # Turn logits into probabilities\n",
        "        probabilities = th.softmax(network(obs), dim=1)[0]\n",
        "        # Into numpy\n",
        "        probabilities = probabilities.detach().cpu().numpy()\n",
        "        # Sample action according to the probabilities\n",
        "        discrete_action = np.random.choice(action_list, p=probabilities)\n",
        "\n",
        "        # Map the discrete action to the corresponding action centroid (vector)\n",
        "        action = action_centroids[discrete_action]\n",
        "        minerl_action = {\"vector\": action}\n",
        "\n",
        "        obs, reward, done, info = env.step(minerl_action)\n",
        "        total_reward += reward\n",
        "        if reward > 0:\n",
        "            rewards.append(reward)\n",
        "        steps += 1\n",
        "        if steps >= MAX_TEST_EPISODE_LEN:\n",
        "            break\n",
        "\n",
        "    env.release()\n",
        "    env.play()\n",
        "    total_rewards.append(total_reward)\n",
        "    print(f'Episode #{episode + 1} reward: {total_reward}\\t\\t episode length: {steps}\\t\\t rewards: {rewards}\\n')\n",
        "\n",
        "avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "print(f'Average dense reward: {avg_total_reward}.')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}